description,best_test_loss,epochs_trained,learning_rate,weight_decay,batch_size,dropout_rate,num_heads,patience,max_epochs
global pooling and dropout in-between layers,0.018615549115176818,249,0.001,0.05,200,0.6,4,20,250
global pooling and dropout last layer,0.020831961620283956,249,0.001,0.05,200,0.6,4,20,250
attention pooling and dropout last layer,0.0006507652063404595,125,0.001,0.05,200,0.6,4,20,250
attention pooling and and dropout in-between layers, 0.0009450079607939468,165,0.001,0.05,200,0.6,4,20,250
