trial,model,combination,window_size,learning_rate,weight_decay,batch_size,dropout_rate,num_heads,num_layers,hidden_channels,out_channels,train_set,test_set,required_feats,additional_feats,epochs,actuals,predictions,perc_completion,sigma,mae,mse,rmse
seed_1,Sage,cartesian_knn50_minmax,20,0.0008648799192603533,5e-05,8,0.5,4,8,256,32,train_dataset.csv,test_dataset.csv,"[""node_positions"", ""node_noise"", ""timestamps"", ""angle_of_arrival""]","[""mean_noise"", ""std_noise"", ""range_noise"", ""dist_to_centroid"", ""sin_azimuth"", ""cos_azimuth"", ""moving_avg_aoa""]","[""Epoch: 0, Train Loss: 3.732264745423849, Val Loss: 2.237525190645720"", ""Epoch: 1, Train Loss: 3.337881065562539, Val Loss: 2.060247367495423"", ""Epoch: 2, Train Loss: 3.308456492064328, Val Loss: 1.808372882696298"", ""Epoch: 3, Train Loss: 2.827135208009303, Val Loss: 1.756581283254779"", ""Epoch: 4, Train Loss: 2.500079905849764, Val Loss: 1.578142392190742""]","[[533.       43.     ]
 [533.       43.     ]
 [533.       43.     ]
 ...
 [902.      877.00006]
 [902.00006 877.     ]
 [902.00006 877.00006]]","[[  89.68472  842.819  ]
 [ 203.41519  924.3602 ]
 [ 168.67346  882.7409 ]
 ...
 [1074.4071   646.5282 ]
 [1074.152    646.0736 ]
 [1074.1469   646.0572 ]]",[0.         0.01371414 0.0273388  ... 0.9629452  0.9814636  1.        ],[3.1234317 3.1234317 3.1234317 ... 5.597543  5.597543  5.597543 ],229.53442,122805.125,350.43562176240016
seed_1,Sage,cartesian_knn50_minmax,20,0.0008648799192603533,5e-05,8,0.5,4,8,256,32,train_dataset.csv,test_dataset.csv,"[""node_positions"", ""node_noise"", ""timestamps"", ""angle_of_arrival""]","[""mean_noise"", ""std_noise"", ""range_noise"", ""dist_to_centroid"", ""sin_azimuth"", ""cos_azimuth"", ""moving_avg_aoa""]","[""Epoch: 0, Train Loss: 3.732264724185412, Val Loss: 2.237525100509326"", ""Epoch: 1, Train Loss: 3.338149825003983, Val Loss: 2.064219549124613"", ""Epoch: 2, Train Loss: 3.300360208612749, Val Loss: 1.786800659897722"", ""Epoch: 3, Train Loss: 2.792013544032629, Val Loss: 1.757944860484772"", ""Epoch: 4, Train Loss: 2.461866374330959, Val Loss: 1.568792539876658""]","[[533.       43.     ]
 [533.       43.     ]
 [533.       43.     ]
 ...
 [902.      877.00006]
 [902.00006 877.     ]
 [902.00006 877.00006]]","[[  71.83072  825.5592 ]
 [ 183.7356   907.47656]
 [ 148.3374   866.5558 ]
 ...
 [1075.2224   646.5447 ]
 [1074.9933   646.14526]
 [1074.8069   645.8191 ]]","[0.12342702 0.23317201 0.34288988 0.4525143  0.56184304 0.67143315
 0.7809375  0.8904753  1.         0.27095687 0.54519564 0.8257823
 0.26697192 0.80041003 0.10906602 0.2545399  0.39985874 0.5452208
 0.6903975  0.836082   0.9817687  0.04082534 0.2041936  0.3674665
 0.5307643  0.6937701  0.85707474 0.03453308 0.17263184 0.3103847
 0.44841814 0.5859482  0.72370565 0.86164796 1.         1.
 0.38464376 1.         0.15535307 0.29338172 0.43117848 0.56935954
 0.70732486 0.84512645 0.98269635 0.49918923 1.         0.14987706
 0.28290367 0.41612962 0.5496737  0.6832108  0.81648386 0.9498484
 0.8567627  0.2864951  0.5716939  0.8570853  0.713682   0.4498676
 1.         0.6919098  0.2387042  0.6190131  1.         1.
 0.8180029  0.17075409 0.36594373 0.56154704 0.7564729  0.95113724
 0.15426946 0.46278885 0.7697253  0.18947965 0.6858791  0.30927244
 0.9223922  0.07152833 0.21412836 0.356997   0.49948058 0.64214826
 0.78528786 0.92872095 0.14727737 0.38255084 0.6175616  0.853129
 0.5007533  0.22704117 0.5909453  0.9546624  0.571479   0.13800731
 0.49032515 0.83082646 0.0548676  0.14266242 0.23059277 0.31841177
 0.40625226 0.4942184  0.5820649  0.6699825  0.7578498  0.8459284
 0.93421894 0.74868226 0.80034477 0.3044008  0.65152705 1.
 0.17595622 0.33287272 0.48974475 0.64698815 0.8038692  0.96071696
 0.33350992 0.7144053  0.49959308 0.6006836  0.4435594  0.21071236
 0.6324796  0.33330315 0.2171746  0.5653063  0.91308296 0.50024813
 0.07145723 0.3561539  0.6439488  0.9287727  0.69947624 0.16329446
 0.37864253 0.5951192  0.81091183 0.5005634  0.58342063 0.4437434
 0.45178565 0.30773044 0.92306644 0.24265926 0.48479882 0.7269546
 0.96961206 0.6156101  0.80113477 0.19979723 0.60026616 1.
 1.         0.2854646  0.11785486 0.35601887 0.59382486 0.82749593
 0.748959   0.26893026 0.57719994 0.88457954 0.1872137  0.43711463
 0.6874388  0.9375753  0.16736542 0.61102206 0.05866703 0.2942801
 0.5293764  0.76469946 1.         0.88870704 0.5002908  1.
 0.36027303 0.68015575 1.         0.31088942 0.58651614 0.86158687
 0.2870995  0.79378754 0.31272116 0.8126866  0.09261179 0.21568932
 0.33863822 0.46145844 0.5846376  0.7076508  0.83063805 0.95378804
 0.1006627  0.23027135 0.35816777 0.48928505 0.61421305 0.7366755
 0.86218655 0.98500127 0.7996203  0.35052338 0.7508694  0.28533804
 0.8574713  0.17985411 0.38457716 0.5898301  0.7947341  1.
 0.75046843 0.19303909 0.45030412 0.70874023 0.9676365  0.33540466
 0.6692543  1.         0.88969046 0.61531425 0.16658567 0.5009548
 0.83401036 0.5548031  0.18550676 0.4822391  0.7781575  0.2142714
 0.7855482  0.06243551 0.14583983 0.2292606  0.31265128 0.3959641
 0.4791758  0.5626463  0.64599437 0.72934675 0.8125243  0.8957769
 0.97913    0.18394871 0.3936468  0.60889405 0.8175709  0.16672668
 0.83327115 0.08063497 0.17253624 0.26445803 0.35650006 0.4484402
 0.5406965  0.63248193 0.72429997 0.81622976 0.9082505  1.
 0.2730812  1.         0.4093728  0.77261114 0.14232174 0.42846698
 0.7147952  1.         0.14573517 0.29111585 0.43632603 0.5819369
 0.7271518  0.87300104 0.18201119 0.9091817  0.5332244  0.25003478
 0.11978658 0.4400484  0.7601377  0.04749194 0.23831674 0.428729
 0.61916095 0.80939686 1.         0.37483782 0.7083858  0.06898321
 0.3441359  0.62033904 0.8966681  0.3337218  0.77779144 0.19248226
 0.49999565 0.80820286 0.03373168 0.12362016 0.21353821 0.3032822
 0.39313662 0.48300073 0.57281774 0.6626203  0.7524936  0.84252864
 0.93239355 0.05984769 0.21974006 0.37942904 0.53936625 0.6992303
 0.85972583 0.12486427 0.6250252  0.75527555 0.36255887 0.72668064
 0.09979197 0.36698502 0.6334032  0.899942   0.4617637  0.09966926
 0.49933386 0.89988977 0.26303145 0.6849267  0.05458096 0.19995755
 0.3455457  0.49091768 0.63633704 0.7817308  0.9271351  0.113506
 0.29514295 0.47679925 0.6587173  0.84102315 0.10542136 0.52696747
 0.947286   0.22869304 0.45704916 0.6856933  0.91429085 0.09532497
 0.22180498 0.3485532  0.475595   0.6025418  0.72967774 0.8569941
 0.98410904 0.80133843 0.29136276 0.62508893 0.9579477  0.20847371
 0.5428013  0.8756366  0.54566145 0.39996153 0.04919449 0.18064623
 0.31175643 0.44260168 0.5737436  0.70502925 0.83621603 0.9671454
 0.5824869  0.66605383 0.20019735 0.42840236 0.6572101  0.8856277
 0.15168831 0.39387506 0.6357662  0.8784664  0.5061748  0.50094104
 0.12899001 0.38772342 0.64589036 0.90329576 0.03294577 0.12080938
 0.20852281 0.29639533 0.38442203 0.4724526  0.5603658  0.6483199
 0.73622763 0.82412595 0.91212547 1.         0.33469957 0.18503605
 0.48167822 0.77744126 0.26598853 1.         0.34629717 0.6541439
 0.9616609  0.08126245 0.17612408 0.27211118 0.3664522  0.46129748
 0.5553715  0.64733607 0.7415057  0.83488834 0.9301066  0.27360615
 1.         0.5006162  0.94473916 0.3642557  0.79086804 0.55639464
 0.35739893 0.92833143 0.16709541 0.33374622 0.50036484 0.66676694
 0.83380914 1.         0.691767   0.8324734  0.69988036 0.46092278
 0.24985836 0.11177807 0.55595785 1.         0.8174005  0.5831532
 0.36390448 0.22269478 1.         0.10587446 0.19993345 0.29410544
 0.38831282 0.48248306 0.57626754 0.67055166 0.76467425 0.8585872
 0.953035   0.36420232 0.0646576  0.32300252 0.58057773 0.83871156
 0.1053789  0.31573093 0.52577585 0.7364588  0.9474595  0.12033454
 0.44005    0.76025873 0.5002372  0.15747702 0.36743397 0.57830644
 0.78915507 1.         0.16699943 0.31543213 0.4633753  0.6113127
 0.7596602  0.90765435 0.40020755 0.18730116 0.6877194  0.25104296
 0.7499456  0.24949427 0.64926445 0.05268757 0.26381007 0.4746227
 0.68453884 0.89479387 0.45506755 0.05533298 0.20396282 0.3521277
 0.500179   0.6481958  0.79636496 0.9445066  1.        ]",[3.1234317 3.1234317 3.1234317 ... 5.597543  5.597543  5.597543 ],227.51108,120700.81,347.4202246559633
